[devops@DEVAD1M ~]$ cat kube-flannel.yaml
apiVersion: v1
kind: Namespace
metadata:
  labels:
    k8s-app: flannel
    pod-security.kubernetes.io/enforce: privileged
  name: kube-flannel
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-app: flannel
  name: flannel
  namespace: kube-flannel
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    k8s-app: flannel
  name: flannel
rules:
- apiGroups:
  - ""
  resources:
  - pods
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - nodes/status
  verbs:
  - patch
- apiGroups:
  - networking.k8s.io
  resources:
  - clustercidrs
  verbs:
  - list
  - watch
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    k8s-app: flannel
  name: flannel
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: flannel
subjects:
- kind: ServiceAccount
  name: flannel
  namespace: kube-flannel
---
apiVersion: v1
data:
  cni-conf.json: |
    {
      "name": "cbr0",
      "cniVersion": "0.3.1",
      "plugins": [
        {
          "type": "flannel",
          "delegate": {
            "hairpinMode": true,
            "isDefaultGateway": true
          }
        },
        {
          "type": "portmap",
          "capabilities": {
            "portMappings": true
          }
        }
      ]
    }
  net-conf.json: |
    {
      "Network": "10.244.0.0/16",
      "Backend": {
        "Type": "vxlan"
      }
    }
kind: ConfigMap
metadata:
  labels:
    app: flannel
    k8s-app: flannel
    tier: node
  name: kube-flannel-cfg
  namespace: kube-flannel
---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  labels:
    app: flannel
    k8s-app: flannel
    tier: node
  name: kube-flannel-ds
  namespace: kube-flannel
spec:
  selector:
    matchLabels:
      app: flannel
      k8s-app: flannel
  template:
    metadata:
      labels:
        app: flannel
        k8s-app: flannel
        tier: node
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/os
                operator: In
                values:
                - linux
      containers:
      - args:
        - --ip-masq
        - --kube-subnet-mgr
        command:
        - /opt/bin/flanneld
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: EVENT_QUEUE_DEPTH
          value: "5000"
        image: 10.173.181.26/flannel/flannel:v0.25.1
        name: kube-flannel
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
        securityContext:
          capabilities:
            add:
            - NET_ADMIN
            - NET_RAW
          privileged: false
        volumeMounts:
        - mountPath: /run/flannel
          name: run
        - mountPath: /etc/kube-flannel/
          name: flannel-cfg
        - mountPath: /run/xtables.lock
          name: xtables-lock
      hostNetwork: true
      initContainers:
      - args:
        - -f
        - /flannel
        - /opt/cni/bin/flannel
        command:
        - cp
        image: 10.173.181.26/flannel/flannel-cni-plugin:v1.4.0-flannel1
        name: install-cni-plugin
        volumeMounts:
        - mountPath: /opt/cni/bin
          name: cni-plugin
      - args:
        - -f
        - /etc/kube-flannel/cni-conf.json
        - /etc/cni/net.d/10-flannel.conflist
        command:
        - cp
        image: 10.173.181.26/flannel/flannel:v0.25.1
        name: install-cni
        volumeMounts:
        - mountPath: /etc/cni/net.d
          name: cni
        - mountPath: /etc/kube-flannel/
          name: flannel-cfg
      priorityClassName: system-node-critical
      serviceAccountName: flannel
      tolerations:
      - effect: NoSchedule
        operator: Exists
      volumes:
      - hostPath:
          path: /run/flannel
        name: run
      - hostPath:
          path: /opt/cni/bin
        name: cni-plugin
      - hostPath:
          path: /etc/cni/net.d
        name: cni
      - configMap:
          name: kube-flannel-cfg
        name: flannel-cfg
      - hostPath:
          path: /run/xtables.lock
          type: FileOrCreate
        name: xtables-lock
[devops@DEVAD1M ~]$ sudo ls -la /etc/cni/net.d/
total 4
drwxr-xr-x  2 root root  33 12 juin  14:09 .
drwxr-xr-x. 3 root root  19 12 juin  13:10 ..
-rw-r--r--  1 root root 292 12 juin  14:09 10-flannel.conflist
[devops@DEVAD1M ~]$ sudo ls -la /opt/cni/bin/
total 57396
drwxr-xr-x. 2 root root    4096 12 juin  14:09 .
drwxr-xr-x. 3 root root      17 11 sept.  2023 ..
-rwxr-xr-x. 1 root root 2868856 11 sept.  2023 bandwidth
-rwxr-xr-x. 1 root root 3248936 11 sept.  2023 bridge
-rwxr-xr-x. 1 root root 8021392 11 sept.  2023 dhcp
-rwxr-xr-x. 1 root root 2971752 11 sept.  2023 dummy
-rwxr-xr-x. 1 root root 3333560 11 sept.  2023 firewall
-rwxr-xr-x  1 root root 2491805 12 juin  14:09 flannel
-rwxr-xr-x. 1 root root 2888056 11 sept.  2023 host-device
-rwxr-xr-x. 1 root root 2426584 11 sept.  2023 host-local
-rwxr-xr-x. 1 root root 2989072 11 sept.  2023 ipvlan
-rwxr-xr-x. 1 root root 2492304 11 sept.  2023 loopback
-rwxr-xr-x. 1 root root 3015104 11 sept.  2023 macvlan
-rwxr-xr-x. 1 root root 2820904 11 sept.  2023 portmap
-rwxr-xr-x. 1 root root 3112536 11 sept.  2023 ptp
-rwxr-xr-x. 1 root root 2642160 11 sept.  2023 sbr
-rwxr-xr-x. 1 root root 2158744 11 sept.  2023 static
-rwxr-xr-x. 1 root root 3035352 11 sept.  2023 tap
-rwxr-xr-x. 1 root root 2556368 11 sept.  2023 tuning
-rwxr-xr-x. 1 root root 2984672 11 sept.  2023 vlan
-rwxr-xr-x. 1 root root 2665880 11 sept.  2023 vrf
[devops@DEVAD1M ~]$ kubectl get pods -A
NAMESPACE      NAME                              READY   STATUS             RESTARTS        AGE
kube-flannel   kube-flannel-ds-lkccq             0/1     CrashLoopBackOff   7 (2m44s ago)   12m
kube-system    coredns-675f547b9c-7zzfh          0/1     Pending            0               23m
kube-system    coredns-76b55c6b58-lcdv2          0/1     Running            0               11m
kube-system    coredns-76b55c6b58-mk7cr          0/1     Running            0               17m
kube-system    etcd-devad1m                      1/1     Running            2               6h7m
kube-system    kube-apiserver-devad1m            1/1     Running            2 (6h8m ago)    6h7m
kube-system    kube-controller-manager-devad1m   1/1     Running            2               6h7m
kube-system    kube-proxy-skl2m                  1/1     Running            0               6h7m
kube-system    kube-scheduler-devad1m            1/1     Running            2               6h7m
[devops@DEVAD1M ~]$ kubectl describe pod coredns -n kube-system
Name:             coredns-675f547b9c-7zzfh
Namespace:        kube-system
Priority:         0
Service Account:  coredns
Node:             <none>
Labels:           k8s-app=kube-dns
                  pod-template-hash=675f547b9c
Annotations:      <none>
Status:           Pending
IP:
IPs:              <none>
Controlled By:    ReplicaSet/coredns-675f547b9c
Containers:
  coredns:
    Image:      10.173.181.26/coredns:v1.11.1
    Port:       <none>
    Host Port:  <none>
    Args:
      -conf
      /etc/coredns/Corefile
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6klzh (ro)
Conditions:
  Type           Status
  PodScheduled   False
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-6klzh:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason            Age                  From               Message
  ----     ------            ----                 ----               -------
  Warning  FailedScheduling  3m10s (x5 over 23m)  default-scheduler  0/1 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.

Name:             coredns-76b55c6b58-lcdv2
Namespace:        kube-system
Priority:         0
Service Account:  coredns
Node:             devad1m/10.173.144.180
Start Time:       Wed, 12 Jun 2024 14:10:30 +0100
Labels:           k8s-app=kube-dns
                  pod-template-hash=76b55c6b58
Annotations:      <none>
Status:           Running
IP:               10.244.0.3
IPs:
  IP:           10.244.0.3
Controlled By:  ReplicaSet/coredns-76b55c6b58
Containers:
  coredns:
    Container ID:  cri-o://fad2be3f905627e7746408b8f99b1b9e4ba91c4d5bf704054c91be64759b2c74
    Image:         10.173.181.26/coredns:v1.11.1
    Image ID:      10.173.181.26/coredns@sha256:54dcce8c8e5073d043ad8d4c4a74ecf4fba40134cc2185da7c697c97a1fa8943
    Port:          <none>
    Host Port:     <none>
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Wed, 12 Jun 2024 14:10:30 +0100
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-sxtrk (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-sxtrk:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason     Age                  From               Message
  ----     ------     ----                 ----               -------
  Normal   Scheduled  11m                  default-scheduler  Successfully assigned kube-system/coredns-76b55c6b58-lcdv2 to devad1m
  Normal   Pulled     11m                  kubelet            Container image "10.173.181.26/coredns:v1.11.1" already present on machine
  Normal   Created    11m                  kubelet            Created container coredns
  Normal   Started    11m                  kubelet            Started container coredns
  Warning  Unhealthy  110s (x68 over 11m)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 503

Name:             coredns-76b55c6b58-mk7cr
Namespace:        kube-system
Priority:         0
Service Account:  coredns
Node:             devad1m/10.173.144.180
Start Time:       Wed, 12 Jun 2024 14:04:46 +0100
Labels:           k8s-app=kube-dns
                  pod-template-hash=76b55c6b58
Annotations:      <none>
Status:           Running
IP:               10.244.0.2
IPs:
  IP:           10.244.0.2
Controlled By:  ReplicaSet/coredns-76b55c6b58
Containers:
  coredns:
    Container ID:  cri-o://689e82022c9d38b7e1f3ba322e0ac220a7996fbedeac61954fd61eb960d83f5d
    Image:         10.173.181.26/coredns:v1.11.1
    Image ID:      10.173.181.26/coredns@sha256:54dcce8c8e5073d043ad8d4c4a74ecf4fba40134cc2185da7c697c97a1fa8943
    Port:          <none>
    Host Port:     <none>
    Args:
      -conf
      /etc/coredns/Corefile
    State:          Running
      Started:      Wed, 12 Jun 2024 14:09:42 +0100
    Ready:          False
    Restart Count:  0
    Limits:
      memory:  170Mi
    Requests:
      cpu:        100m
      memory:     70Mi
    Liveness:     http-get http://:8080/health delay=60s timeout=5s period=10s #success=1 #failure=5
    Readiness:    http-get http://:8181/ready delay=0s timeout=1s period=10s #success=1 #failure=3
    Environment:  <none>
    Mounts:
      /etc/coredns from config-volume (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-q7lfp (ro)
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  config-volume:
    Type:      ConfigMap (a volume populated by a ConfigMap)
    Name:      coredns
    Optional:  false
  kube-api-access-q7lfp:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node-role.kubernetes.io/control-plane:NoSchedule op=Exists
                             node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason                  Age                  From               Message
  ----     ------                  ----                 ----               -------
  Normal   Scheduled               17m                  default-scheduler  Successfully assigned kube-system/coredns-76b55c6b58-mk7cr to devad1m
  Warning  FailedCreatePodSandBox  17m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-76b55c6b58-mk7cr_kube-system_e0244d0a-45b4-40dd-bcea-a30e37fa34b5_0(b97964fd5361b6120e47d375b10e7e2105d55b1567c8424bb0bc7a00e0e4963f): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?
  Warning  FailedCreatePodSandBox  17m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-76b55c6b58-mk7cr_kube-system_e0244d0a-45b4-40dd-bcea-a30e37fa34b5_0(7ef7a449287afc267333584e5428fd58b3b3864fb4d35c67e1f12919518d813c): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?
  Warning  FailedCreatePodSandBox  17m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-76b55c6b58-mk7cr_kube-system_e0244d0a-45b4-40dd-bcea-a30e37fa34b5_0(f8839b4b88392f55b7e02906367e5d746c3561679e78cb532ff0fb7f6d70772d): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?
  Warning  FailedCreatePodSandBox  16m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-76b55c6b58-mk7cr_kube-system_e0244d0a-45b4-40dd-bcea-a30e37fa34b5_0(8f3114f8db1f69f424520328565c78f1ce9b1bb3f08e4e8f9a2d53cd8b82befd): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?
  Warning  FailedCreatePodSandBox  16m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-76b55c6b58-mk7cr_kube-system_e0244d0a-45b4-40dd-bcea-a30e37fa34b5_0(57349972c3bfb1508f5219139a3770ecc8558c359f9e9fc4f49e785adafb8d31): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?
  Warning  FailedCreatePodSandBox  16m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-76b55c6b58-mk7cr_kube-system_e0244d0a-45b4-40dd-bcea-a30e37fa34b5_0(0e98bb7e410d5d5dc185f8decad1d7272b91548355c5c9575309f6425595298f): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?
  Warning  FailedCreatePodSandBox  16m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-76b55c6b58-mk7cr_kube-system_e0244d0a-45b4-40dd-bcea-a30e37fa34b5_0(885ff1dc2247506f0f33eb6a43657cbe2bfe7c6c2e3513a9e5e59dcd9ca020c3): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?
  Warning  FailedCreatePodSandBox  16m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-76b55c6b58-mk7cr_kube-system_e0244d0a-45b4-40dd-bcea-a30e37fa34b5_0(5dbbbd22fbd331bb74544efd40e6dabf2d9053e79d376a7d098e598749b7cb93): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?
  Warning  FailedCreatePodSandBox  15m                  kubelet            Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-76b55c6b58-mk7cr_kube-system_e0244d0a-45b4-40dd-bcea-a30e37fa34b5_0(41061e0a1eb416a8880250e1ec6a82e694059f1f5239dd9659089d04f8155db8): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?
  Warning  FailedCreatePodSandBox  12m (x13 over 15m)   kubelet            (combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to create pod network sandbox k8s_coredns-76b55c6b58-mk7cr_kube-system_e0244d0a-45b4-40dd-bcea-a30e37fa34b5_0(48ed01087b983a745232b77b5e5f8cc4097bef393ba033f3c2d3205056662a78): No CNI configuration file in /etc/cni/net.d/. Has your network provider started?
  Normal   Pulled                  12m                  kubelet            Container image "10.173.181.26/coredns:v1.11.1" already present on machine
  Normal   Created                 12m                  kubelet            Created container coredns
  Normal   Started                 12m                  kubelet            Started container coredns
  Warning  Unhealthy               12m (x4 over 12m)    kubelet            Readiness probe failed: HTTP probe failed with statuscode: 503
  Warning  Unhealthy               109s (x69 over 11m)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 503


